{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b16d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from dataset import PeptideDataset\n",
    "from model import RetentionPredictor\n",
    "from utils import evaluate_regression_metrics\n",
    "from train_cmaes import train_cmaes_1_1\n",
    "from train_de import train_de\n",
    "from train_adam import train_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df110c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\"))\n",
    "SEED = int(os.getenv(\"RANDOM_SEED\"))\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a268853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd46cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZES = [32, 16]\n",
    "RANDOM_SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#DATASETS = [\"../data/krokhin.txt\", \"../data/mouse.txt\" \"../data/petritis.txt\", \"../data/serum.txt\"]\n",
    "\n",
    "ADAM_CONFIG = {\"epochs\": 150, \"lr\": 3e-4}\n",
    "CMAES_CONFIG = {\"generations\": 400}\n",
    "DE_CONFIG = {\"generations\": 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054dc53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e149f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    sequences, retention_times = [], []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                seq, rt = parts\n",
    "                sequences.append(seq)\n",
    "                retention_times.append(float(rt))\n",
    "    dataset = PeptideDataset(sequences, retention_times)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    return torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7af94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(algorithm, dataset_path, config):\n",
    "    set_seed(RANDOM_SEED)\n",
    "    train_data, val_data = load_data(dataset_path)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = RetentionPredictor()\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    if algorithm == \"adam\":\n",
    "        model, history = train_adam(model, criterion, train_loader, val_loader, **config)\n",
    "    elif algorithm == \"cmaes\":\n",
    "        model, history = train_cmaes_1_1(model, criterion, train_loader, val_loader, **config)\n",
    "    elif algorithm == \"de\":\n",
    "        model, history = train_de(model, criterion, train_loader, val_loader, **config)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown algorithm\")\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    metrics = evaluate_regression_metrics(model, val_loader)\n",
    "\n",
    "    return {\n",
    "        \"algorithm\": algorithm,\n",
    "        \"dataset_name\": os.path.basename(dataset_path),\n",
    "        \"history\": history,\n",
    "        \"final_metrics\": {\n",
    "            **metrics,\n",
    "            \"train_loss\": history[\"train_loss\"][-1],\n",
    "            \"val_loss\": history[\"val_loss\"][-1],\n",
    "            \"eval_calls\": history[\"eval_calls\"],\n",
    "            \"train_time\": elapsed\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6ce167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label):\n",
    "    plt.plot(history[\"generation\"], history[\"train_loss\"], label=f\"Train - {label}\")\n",
    "    plt.plot(history[\"generation\"], history[\"val_loss\"], label=f\"Val - {label}\")\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "023ae90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(res):\n",
    "    print(\"\\n\\nFinal Metrics Summary:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"{res['algorithm'].upper()} | Dataset: {res['dataset_name']} | Val R2: {res['final_metrics']['r2']:.3f} | MAE: {res['final_metrics']['mae']:.4f} | Eval Calls: {res['final_metrics']['eval_calls']} | Time: {res['final_metrics']['train_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1b8b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset: data/krokhin.txt =====\n",
      "Running ADAM...\n",
      "Epoch   1/150 | Train Loss: 0.07396 | Val Loss: 0.07326\n",
      "Epoch   2/150 | Train Loss: 0.04961 | Val Loss: 0.05050\n",
      "Epoch   3/150 | Train Loss: 0.03556 | Val Loss: 0.03431\n",
      "Epoch   4/150 | Train Loss: 0.02514 | Val Loss: 0.02367\n",
      "Epoch   5/150 | Train Loss: 0.01969 | Val Loss: 0.01727\n",
      "Epoch   6/150 | Train Loss: 0.01579 | Val Loss: 0.01378\n",
      "Epoch   7/150 | Train Loss: 0.01277 | Val Loss: 0.01151\n",
      "Epoch   8/150 | Train Loss: 0.01086 | Val Loss: 0.01009\n",
      "Epoch   9/150 | Train Loss: 0.00908 | Val Loss: 0.00911\n",
      "Epoch  10/150 | Train Loss: 0.00762 | Val Loss: 0.00837\n",
      "Epoch  11/150 | Train Loss: 0.00680 | Val Loss: 0.00764\n",
      "Epoch  12/150 | Train Loss: 0.00590 | Val Loss: 0.00711\n",
      "Epoch  13/150 | Train Loss: 0.00547 | Val Loss: 0.00667\n",
      "Epoch  14/150 | Train Loss: 0.00503 | Val Loss: 0.00633\n",
      "Epoch  15/150 | Train Loss: 0.00470 | Val Loss: 0.00603\n",
      "Epoch  16/150 | Train Loss: 0.00451 | Val Loss: 0.00576\n",
      "Epoch  17/150 | Train Loss: 0.00411 | Val Loss: 0.00548\n",
      "Epoch  18/150 | Train Loss: 0.00383 | Val Loss: 0.00523\n",
      "Epoch  19/150 | Train Loss: 0.00380 | Val Loss: 0.00500\n",
      "Epoch  20/150 | Train Loss: 0.00361 | Val Loss: 0.00480\n",
      "Epoch  21/150 | Train Loss: 0.00334 | Val Loss: 0.00458\n",
      "Epoch  22/150 | Train Loss: 0.00315 | Val Loss: 0.00435\n",
      "Epoch  23/150 | Train Loss: 0.00290 | Val Loss: 0.00415\n",
      "Epoch  24/150 | Train Loss: 0.00281 | Val Loss: 0.00396\n",
      "Epoch  25/150 | Train Loss: 0.00253 | Val Loss: 0.00380\n",
      "Epoch  26/150 | Train Loss: 0.00232 | Val Loss: 0.00364\n",
      "Epoch  27/150 | Train Loss: 0.00229 | Val Loss: 0.00350\n",
      "Epoch  28/150 | Train Loss: 0.00224 | Val Loss: 0.00334\n",
      "Epoch  29/150 | Train Loss: 0.00199 | Val Loss: 0.00317\n",
      "Epoch  30/150 | Train Loss: 0.00197 | Val Loss: 0.00305\n",
      "Epoch  31/150 | Train Loss: 0.00183 | Val Loss: 0.00292\n",
      "Epoch  32/150 | Train Loss: 0.00177 | Val Loss: 0.00283\n",
      "Epoch  33/150 | Train Loss: 0.00177 | Val Loss: 0.00271\n",
      "Epoch  34/150 | Train Loss: 0.00160 | Val Loss: 0.00259\n",
      "Epoch  35/150 | Train Loss: 0.00148 | Val Loss: 0.00249\n",
      "Epoch  36/150 | Train Loss: 0.00142 | Val Loss: 0.00240\n",
      "Epoch  37/150 | Train Loss: 0.00141 | Val Loss: 0.00234\n",
      "Epoch  38/150 | Train Loss: 0.00139 | Val Loss: 0.00224\n",
      "Epoch  39/150 | Train Loss: 0.00127 | Val Loss: 0.00218\n",
      "Epoch  40/150 | Train Loss: 0.00119 | Val Loss: 0.00210\n",
      "Epoch  41/150 | Train Loss: 0.00121 | Val Loss: 0.00206\n",
      "Epoch  42/150 | Train Loss: 0.00113 | Val Loss: 0.00202\n",
      "Epoch  43/150 | Train Loss: 0.00115 | Val Loss: 0.00194\n",
      "Epoch  44/150 | Train Loss: 0.00110 | Val Loss: 0.00191\n",
      "Epoch  45/150 | Train Loss: 0.00106 | Val Loss: 0.00190\n",
      "Epoch  46/150 | Train Loss: 0.00103 | Val Loss: 0.00185\n",
      "Epoch  47/150 | Train Loss: 0.00094 | Val Loss: 0.00179\n",
      "Epoch  48/150 | Train Loss: 0.00097 | Val Loss: 0.00175\n",
      "Epoch  49/150 | Train Loss: 0.00093 | Val Loss: 0.00173\n",
      "Epoch  50/150 | Train Loss: 0.00091 | Val Loss: 0.00172\n",
      "Epoch  51/150 | Train Loss: 0.00091 | Val Loss: 0.00168\n",
      "Epoch  52/150 | Train Loss: 0.00085 | Val Loss: 0.00170\n",
      "Epoch  53/150 | Train Loss: 0.00086 | Val Loss: 0.00169\n",
      "Epoch  54/150 | Train Loss: 0.00088 | Val Loss: 0.00159\n",
      "Epoch  55/150 | Train Loss: 0.00083 | Val Loss: 0.00158\n",
      "Epoch  56/150 | Train Loss: 0.00080 | Val Loss: 0.00160\n",
      "Epoch  57/150 | Train Loss: 0.00079 | Val Loss: 0.00159\n",
      "Epoch  58/150 | Train Loss: 0.00078 | Val Loss: 0.00153\n",
      "Epoch  59/150 | Train Loss: 0.00075 | Val Loss: 0.00152\n",
      "Epoch  60/150 | Train Loss: 0.00074 | Val Loss: 0.00154\n",
      "Epoch  61/150 | Train Loss: 0.00077 | Val Loss: 0.00149\n",
      "Epoch  62/150 | Train Loss: 0.00073 | Val Loss: 0.00149\n",
      "Epoch  63/150 | Train Loss: 0.00070 | Val Loss: 0.00147\n",
      "Epoch  64/150 | Train Loss: 0.00068 | Val Loss: 0.00148\n",
      "Epoch  65/150 | Train Loss: 0.00068 | Val Loss: 0.00148\n",
      "Epoch  66/150 | Train Loss: 0.00070 | Val Loss: 0.00145\n",
      "Epoch  67/150 | Train Loss: 0.00071 | Val Loss: 0.00146\n",
      "Epoch  68/150 | Train Loss: 0.00065 | Val Loss: 0.00148\n",
      "Epoch  69/150 | Train Loss: 0.00064 | Val Loss: 0.00143\n",
      "Epoch  70/150 | Train Loss: 0.00062 | Val Loss: 0.00143\n",
      "Epoch  71/150 | Train Loss: 0.00061 | Val Loss: 0.00144\n",
      "Epoch  72/150 | Train Loss: 0.00061 | Val Loss: 0.00141\n",
      "Epoch  73/150 | Train Loss: 0.00063 | Val Loss: 0.00141\n",
      "Epoch  74/150 | Train Loss: 0.00059 | Val Loss: 0.00141\n",
      "Epoch  75/150 | Train Loss: 0.00061 | Val Loss: 0.00140\n",
      "Epoch  76/150 | Train Loss: 0.00058 | Val Loss: 0.00138\n",
      "Epoch  77/150 | Train Loss: 0.00058 | Val Loss: 0.00142\n",
      "Epoch  78/150 | Train Loss: 0.00056 | Val Loss: 0.00139\n",
      "Epoch  79/150 | Train Loss: 0.00056 | Val Loss: 0.00136\n",
      "Epoch  80/150 | Train Loss: 0.00054 | Val Loss: 0.00137\n",
      "Epoch  81/150 | Train Loss: 0.00056 | Val Loss: 0.00140\n",
      "Epoch  82/150 | Train Loss: 0.00052 | Val Loss: 0.00142\n",
      "Epoch  83/150 | Train Loss: 0.00052 | Val Loss: 0.00141\n",
      "Epoch  84/150 | Train Loss: 0.00052 | Val Loss: 0.00140\n",
      "Epoch  85/150 | Train Loss: 0.00052 | Val Loss: 0.00137\n",
      "Epoch  86/150 | Train Loss: 0.00057 | Val Loss: 0.00141\n",
      "Epoch  87/150 | Train Loss: 0.00048 | Val Loss: 0.00140\n",
      "Epoch  88/150 | Train Loss: 0.00049 | Val Loss: 0.00145\n",
      "Epoch  89/150 | Train Loss: 0.00048 | Val Loss: 0.00136\n",
      "Epoch  90/150 | Train Loss: 0.00046 | Val Loss: 0.00136\n",
      "Epoch  91/150 | Train Loss: 0.00048 | Val Loss: 0.00139\n",
      "Epoch  92/150 | Train Loss: 0.00046 | Val Loss: 0.00139\n",
      "Epoch  93/150 | Train Loss: 0.00046 | Val Loss: 0.00134\n",
      "Epoch  94/150 | Train Loss: 0.00044 | Val Loss: 0.00134\n",
      "Epoch  95/150 | Train Loss: 0.00046 | Val Loss: 0.00135\n",
      "Epoch  96/150 | Train Loss: 0.00044 | Val Loss: 0.00134\n",
      "Epoch  97/150 | Train Loss: 0.00044 | Val Loss: 0.00135\n",
      "Epoch  98/150 | Train Loss: 0.00043 | Val Loss: 0.00133\n",
      "Epoch  99/150 | Train Loss: 0.00041 | Val Loss: 0.00135\n",
      "Epoch 100/150 | Train Loss: 0.00043 | Val Loss: 0.00134\n",
      "Epoch 101/150 | Train Loss: 0.00041 | Val Loss: 0.00132\n",
      "Epoch 102/150 | Train Loss: 0.00042 | Val Loss: 0.00134\n",
      "Epoch 103/150 | Train Loss: 0.00043 | Val Loss: 0.00135\n",
      "Epoch 104/150 | Train Loss: 0.00043 | Val Loss: 0.00133\n",
      "Epoch 105/150 | Train Loss: 0.00041 | Val Loss: 0.00134\n",
      "Epoch 106/150 | Train Loss: 0.00039 | Val Loss: 0.00137\n",
      "Epoch 107/150 | Train Loss: 0.00039 | Val Loss: 0.00131\n",
      "Epoch 108/150 | Train Loss: 0.00038 | Val Loss: 0.00130\n",
      "Epoch 109/150 | Train Loss: 0.00038 | Val Loss: 0.00135\n",
      "Epoch 110/150 | Train Loss: 0.00038 | Val Loss: 0.00134\n",
      "Epoch 111/150 | Train Loss: 0.00037 | Val Loss: 0.00129\n",
      "Epoch 112/150 | Train Loss: 0.00038 | Val Loss: 0.00135\n",
      "Epoch 113/150 | Train Loss: 0.00038 | Val Loss: 0.00130\n",
      "Epoch 114/150 | Train Loss: 0.00037 | Val Loss: 0.00131\n",
      "Epoch 115/150 | Train Loss: 0.00037 | Val Loss: 0.00133\n",
      "Epoch 116/150 | Train Loss: 0.00042 | Val Loss: 0.00129\n",
      "Epoch 117/150 | Train Loss: 0.00035 | Val Loss: 0.00138\n",
      "Epoch 118/150 | Train Loss: 0.00036 | Val Loss: 0.00131\n",
      "Epoch 119/150 | Train Loss: 0.00035 | Val Loss: 0.00131\n",
      "Epoch 120/150 | Train Loss: 0.00033 | Val Loss: 0.00136\n",
      "Epoch 121/150 | Train Loss: 0.00035 | Val Loss: 0.00129\n",
      "Epoch 122/150 | Train Loss: 0.00036 | Val Loss: 0.00134\n",
      "Epoch 123/150 | Train Loss: 0.00035 | Val Loss: 0.00133\n",
      "Epoch 124/150 | Train Loss: 0.00033 | Val Loss: 0.00130\n",
      "Epoch 125/150 | Train Loss: 0.00033 | Val Loss: 0.00135\n",
      "Epoch 126/150 | Train Loss: 0.00031 | Val Loss: 0.00130\n",
      "Epoch 127/150 | Train Loss: 0.00033 | Val Loss: 0.00132\n",
      "Epoch 128/150 | Train Loss: 0.00032 | Val Loss: 0.00136\n",
      "Epoch 129/150 | Train Loss: 0.00031 | Val Loss: 0.00133\n",
      "Epoch 130/150 | Train Loss: 0.00032 | Val Loss: 0.00134\n",
      "Epoch 131/150 | Train Loss: 0.00030 | Val Loss: 0.00134\n",
      "Epoch 132/150 | Train Loss: 0.00030 | Val Loss: 0.00133\n",
      "Epoch 133/150 | Train Loss: 0.00029 | Val Loss: 0.00133\n",
      "Epoch 134/150 | Train Loss: 0.00031 | Val Loss: 0.00133\n",
      "Epoch 135/150 | Train Loss: 0.00030 | Val Loss: 0.00136\n",
      "Epoch 136/150 | Train Loss: 0.00030 | Val Loss: 0.00132\n",
      "Epoch 137/150 | Train Loss: 0.00030 | Val Loss: 0.00136\n",
      "Epoch 138/150 | Train Loss: 0.00028 | Val Loss: 0.00131\n",
      "Epoch 139/150 | Train Loss: 0.00028 | Val Loss: 0.00132\n",
      "Epoch 140/150 | Train Loss: 0.00029 | Val Loss: 0.00133\n",
      "Epoch 141/150 | Train Loss: 0.00028 | Val Loss: 0.00131\n",
      "Epoch 142/150 | Train Loss: 0.00028 | Val Loss: 0.00132\n",
      "Epoch 143/150 | Train Loss: 0.00028 | Val Loss: 0.00133\n",
      "Epoch 144/150 | Train Loss: 0.00029 | Val Loss: 0.00133\n",
      "Epoch 145/150 | Train Loss: 0.00027 | Val Loss: 0.00135\n",
      "Epoch 146/150 | Train Loss: 0.00029 | Val Loss: 0.00133\n",
      "Epoch 147/150 | Train Loss: 0.00027 | Val Loss: 0.00134\n",
      "Epoch 148/150 | Train Loss: 0.00027 | Val Loss: 0.00129\n",
      "Epoch 149/150 | Train Loss: 0.00026 | Val Loss: 0.00135\n",
      "Epoch 150/150 | Train Loss: 0.00027 | Val Loss: 0.00135\n",
      "\n",
      "\n",
      "Final Metrics Summary:\n",
      "========================================\n",
      "ADAM | Dataset: krokhin.txt | Val R2: 0.917 | MAE: 0.0420 | Eval Calls: 150 | Time: 8.24s\n",
      "Running CMAES...\n",
      "Gen   1/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.49539 | Success: False\n",
      "Gen   2/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.48668 | Success: False\n",
      "Gen   3/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.47442 | Success: False\n",
      "Gen   4/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.45917 | Success: False\n",
      "Gen   5/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.44153 | Success: False\n",
      "Gen   6/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.42202 | Success: False\n",
      "Gen   7/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.40117 | Success: False\n",
      "Gen   8/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.37943 | Success: False\n",
      "Gen   9/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.35722 | Success: False\n",
      "Gen  10/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.33488 | Success: False\n",
      "Gen  11/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.31273 | Success: False\n",
      "Gen  12/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.29101 | Success: False\n",
      "Gen  13/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.26992 | Success: False\n",
      "Gen  14/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.24960 | Success: False\n",
      "Gen  15/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.23019 | Success: False\n",
      "Gen  16/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.21175 | Success: False\n",
      "Gen  17/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.19434 | Success: False\n",
      "Gen  18/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.17799 | Success: False\n",
      "Gen  19/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.16270 | Success: False\n",
      "Gen  20/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.14846 | Success: False\n",
      "Gen  21/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.13524 | Success: False\n",
      "Gen  22/500 | Train Loss: 0.08589 | Val Loss: 0.10267 | σ: 0.12302 | Success: False\n",
      "Gen  23/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.11759 | Success: True\n",
      "Gen  24/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.11178 | Success: False\n",
      "Gen  25/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.10573 | Success: False\n",
      "Gen  26/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.09954 | Success: False\n",
      "Gen  27/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.09332 | Success: False\n",
      "Gen  28/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.08715 | Success: False\n",
      "Gen  29/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.08110 | Success: False\n",
      "Gen  30/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.07522 | Success: False\n",
      "Gen  31/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.06956 | Success: False\n",
      "Gen  32/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.06415 | Success: False\n",
      "Gen  33/500 | Train Loss: 0.03208 | Val Loss: 0.03363 | σ: 0.05901 | Success: False\n",
      "Gen  34/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.05699 | Success: True\n",
      "Gen  35/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.05469 | Success: False\n",
      "Gen  36/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.05218 | Success: False\n",
      "Gen  37/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.04952 | Success: False\n",
      "Gen  38/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.04676 | Success: False\n",
      "Gen  39/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.04396 | Success: False\n",
      "Gen  40/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.04116 | Success: False\n",
      "Gen  41/500 | Train Loss: 0.03082 | Val Loss: 0.03671 | σ: 0.03839 | Success: False\n",
      "Gen  42/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.03755 | Success: True\n",
      "Gen  43/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.03645 | Success: False\n",
      "Gen  44/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.03515 | Success: False\n",
      "Gen  45/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.03369 | Success: False\n",
      "Gen  46/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.03210 | Success: False\n",
      "Gen  47/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.03042 | Success: False\n",
      "Gen  48/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.02870 | Success: False\n",
      "Gen  49/500 | Train Loss: 0.02523 | Val Loss: 0.02754 | σ: 0.02696 | Success: False\n",
      "Gen  50/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.02653 | Success: True\n",
      "Gen  51/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.02591 | Success: False\n",
      "Gen  52/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.02512 | Success: False\n",
      "Gen  53/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.02419 | Success: False\n",
      "Gen  54/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.02315 | Success: False\n",
      "Gen  55/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.02203 | Success: False\n",
      "Gen  56/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.02086 | Success: False\n",
      "Gen  57/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01966 | Success: False\n",
      "Gen  58/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01845 | Success: False\n",
      "Gen  59/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01724 | Success: False\n",
      "Gen  60/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01606 | Success: False\n",
      "Gen  61/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01491 | Success: False\n",
      "Gen  62/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01379 | Success: False\n",
      "Gen  63/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01273 | Success: False\n",
      "Gen  64/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01172 | Success: False\n",
      "Gen  65/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.01076 | Success: False\n",
      "Gen  66/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00986 | Success: False\n",
      "Gen  67/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00901 | Success: False\n",
      "Gen  68/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00823 | Success: False\n",
      "Gen  69/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00750 | Success: False\n",
      "Gen  70/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00682 | Success: False\n",
      "Gen  71/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00620 | Success: False\n",
      "Gen  72/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00563 | Success: False\n",
      "Gen  73/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00510 | Success: False\n",
      "Gen  74/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00462 | Success: False\n",
      "Gen  75/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00418 | Success: False\n",
      "Gen  76/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00378 | Success: False\n",
      "Gen  77/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00341 | Success: False\n",
      "Gen  78/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00308 | Success: False\n",
      "Gen  79/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00277 | Success: False\n",
      "Gen  80/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00250 | Success: False\n",
      "Gen  81/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00225 | Success: False\n",
      "Gen  82/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00203 | Success: False\n",
      "Gen  83/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00182 | Success: False\n",
      "Gen  84/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00164 | Success: False\n",
      "Gen  85/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00147 | Success: False\n",
      "Gen  86/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00132 | Success: False\n",
      "Gen  87/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00119 | Success: False\n",
      "Gen  88/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00107 | Success: False\n",
      "Gen  89/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00096 | Success: False\n",
      "Gen  90/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00086 | Success: False\n",
      "Gen  91/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00077 | Success: False\n",
      "Gen  92/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00069 | Success: False\n",
      "Gen  93/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00062 | Success: False\n",
      "Gen  94/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00056 | Success: False\n",
      "Gen  95/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00050 | Success: False\n",
      "Gen  96/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00045 | Success: False\n",
      "Gen  97/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00040 | Success: False\n",
      "Gen  98/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00036 | Success: False\n",
      "Gen  99/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00032 | Success: False\n",
      "Gen 100/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00029 | Success: False\n",
      "Gen 101/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00026 | Success: False\n",
      "Gen 102/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00023 | Success: False\n",
      "Gen 103/500 | Train Loss: 0.02345 | Val Loss: 0.02532 | σ: 0.00021 | Success: False\n",
      "Gen 104/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00020 | Success: True\n",
      "Gen 105/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00018 | Success: False\n",
      "Gen 106/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00017 | Success: False\n",
      "Gen 107/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00016 | Success: False\n",
      "Gen 108/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00015 | Success: False\n",
      "Gen 109/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00014 | Success: False\n",
      "Gen 110/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00013 | Success: False\n",
      "Gen 111/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00012 | Success: False\n",
      "Gen 112/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00011 | Success: False\n",
      "Gen 113/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00010 | Success: False\n",
      "Gen 114/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00009 | Success: False\n",
      "Gen 115/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00008 | Success: False\n",
      "Gen 116/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00007 | Success: False\n",
      "Gen 117/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00007 | Success: False\n",
      "Gen 118/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00006 | Success: False\n",
      "Gen 119/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00006 | Success: False\n",
      "Gen 120/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00005 | Success: False\n",
      "Gen 121/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00005 | Success: False\n",
      "Gen 122/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00004 | Success: False\n",
      "Gen 123/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00004 | Success: False\n",
      "Gen 124/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00003 | Success: False\n",
      "Gen 125/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00003 | Success: False\n",
      "Gen 126/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00003 | Success: False\n",
      "Gen 127/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00002 | Success: False\n",
      "Gen 128/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00002 | Success: False\n",
      "Gen 129/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00002 | Success: False\n",
      "Gen 130/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00002 | Success: False\n",
      "Gen 131/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00002 | Success: False\n",
      "Gen 132/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 133/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 134/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 135/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 136/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 137/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 138/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 139/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 140/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 141/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00001 | Success: False\n",
      "Gen 142/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 143/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 144/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 145/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 146/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 147/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 148/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 149/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 150/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 151/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 152/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 153/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 154/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 155/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 156/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Gen 157/500 | Train Loss: 0.02292 | Val Loss: 0.02493 | σ: 0.00000 | Success: False\n",
      "Early stop: sigma ≈ 0\n",
      "\n",
      "\n",
      "Final Metrics Summary:\n",
      "========================================\n",
      "CMAES | Dataset: krokhin.txt | Val R2: -0.531 | MAE: 0.1807 | Eval Calls: 314 | Time: 7.60s\n",
      "Running DE...\n",
      "Starting DE: 3,457 parameters | pop=20 | F=0.5 | CR=0.9 | generations=400\n",
      "Initial best train loss = 0.01965\n",
      "Generation   1/400 | Population mean: 0.06777 | Best train loss: 0.01965 | Val loss: 0.01597 | Pop. STD: 0.151081\n",
      "Generation   2/400 | Population mean: 0.05020 | Best train loss: 0.01965 | Val loss: 0.01597 | Pop. STD: 0.155412\n",
      "Generation   3/400 | Population mean: 0.04735 | Best train loss: 0.01965 | Val loss: 0.01597 | Pop. STD: 0.155594\n",
      "Generation   4/400 | Population mean: 0.04664 | Best train loss: 0.01965 | Val loss: 0.01597 | Pop. STD: 0.155626\n",
      "[gen 004] new best train loss = 0.018957\n",
      "Generation   5/400 | Population mean: 0.03744 | Best train loss: 0.01896 | Val loss: 0.01484 | Pop. STD: 0.150950\n",
      "[gen 005] new best train loss = 0.017354\n",
      "Generation   6/400 | Population mean: 0.03477 | Best train loss: 0.01735 | Val loss: 0.01642 | Pop. STD: 0.150522\n",
      "[gen 006] new best train loss = 0.014330\n",
      "Generation   7/400 | Population mean: 0.03200 | Best train loss: 0.01433 | Val loss: 0.01251 | Pop. STD: 0.149980\n",
      "Generation   8/400 | Population mean: 0.03200 | Best train loss: 0.01433 | Val loss: 0.01251 | Pop. STD: 0.149980\n",
      "Generation   9/400 | Population mean: 0.03139 | Best train loss: 0.01433 | Val loss: 0.01251 | Pop. STD: 0.150876\n",
      "Generation  10/400 | Population mean: 0.03016 | Best train loss: 0.01433 | Val loss: 0.01251 | Pop. STD: 0.150345\n",
      "Generation  11/400 | Population mean: 0.02821 | Best train loss: 0.01433 | Val loss: 0.01251 | Pop. STD: 0.150199\n",
      "Generation  12/400 | Population mean: 0.02821 | Best train loss: 0.01433 | Val loss: 0.01251 | Pop. STD: 0.150199\n",
      "[gen 012] new best train loss = 0.014048\n",
      "Generation  13/400 | Population mean: 0.02713 | Best train loss: 0.01405 | Val loss: 0.01009 | Pop. STD: 0.148810\n",
      "Generation  14/400 | Population mean: 0.02258 | Best train loss: 0.01405 | Val loss: 0.01009 | Pop. STD: 0.146548\n",
      "[gen 014] new best train loss = 0.010635\n",
      "Generation  15/400 | Population mean: 0.02096 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.148128\n",
      "Generation  16/400 | Population mean: 0.02056 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.147314\n",
      "Generation  17/400 | Population mean: 0.02056 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.147314\n",
      "Generation  18/400 | Population mean: 0.02043 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.148074\n",
      "Generation  19/400 | Population mean: 0.02001 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.147146\n",
      "Generation  20/400 | Population mean: 0.01938 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.147054\n",
      "Generation  21/400 | Population mean: 0.01898 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.146053\n",
      "Generation  22/400 | Population mean: 0.01778 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.145341\n",
      "Generation  23/400 | Population mean: 0.01677 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.145547\n",
      "Generation  24/400 | Population mean: 0.01657 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.144767\n",
      "Generation  25/400 | Population mean: 0.01561 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.144021\n",
      "Generation  26/400 | Population mean: 0.01515 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.142661\n",
      "Generation  27/400 | Population mean: 0.01502 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.142403\n",
      "Generation  28/400 | Population mean: 0.01393 | Best train loss: 0.01064 | Val loss: 0.00733 | Pop. STD: 0.144240\n",
      "[gen 028] new best train loss = 0.010294\n",
      "Generation  29/400 | Population mean: 0.01353 | Best train loss: 0.01029 | Val loss: 0.00840 | Pop. STD: 0.143772\n",
      "Generation  30/400 | Population mean: 0.01314 | Best train loss: 0.01029 | Val loss: 0.00840 | Pop. STD: 0.143622\n",
      "Generation  31/400 | Population mean: 0.01307 | Best train loss: 0.01029 | Val loss: 0.00840 | Pop. STD: 0.144167\n",
      "[gen 031] new best train loss = 0.009243\n",
      "Generation  32/400 | Population mean: 0.01267 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.143356\n",
      "Generation  33/400 | Population mean: 0.01244 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.143295\n",
      "Generation  34/400 | Population mean: 0.01224 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.144421\n",
      "Generation  35/400 | Population mean: 0.01223 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.144724\n",
      "Generation  36/400 | Population mean: 0.01203 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.146043\n",
      "Generation  37/400 | Population mean: 0.01201 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.146164\n",
      "Generation  38/400 | Population mean: 0.01185 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.146476\n",
      "Generation  39/400 | Population mean: 0.01184 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.146064\n",
      "Generation  40/400 | Population mean: 0.01166 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.146522\n",
      "Generation  41/400 | Population mean: 0.01166 | Best train loss: 0.00924 | Val loss: 0.01147 | Pop. STD: 0.146522\n",
      "[gen 041] new best train loss = 0.009170\n",
      "Generation  42/400 | Population mean: 0.01141 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.146409\n",
      "Generation  43/400 | Population mean: 0.01139 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.145361\n",
      "Generation  44/400 | Population mean: 0.01133 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.145605\n",
      "Generation  45/400 | Population mean: 0.01114 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.145558\n",
      "Generation  46/400 | Population mean: 0.01114 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.145558\n",
      "Generation  47/400 | Population mean: 0.01110 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.145474\n",
      "Generation  48/400 | Population mean: 0.01110 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.145474\n",
      "Generation  49/400 | Population mean: 0.01102 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.146305\n",
      "Generation  50/400 | Population mean: 0.01088 | Best train loss: 0.00917 | Val loss: 0.01617 | Pop. STD: 0.146568\n",
      "[gen 050] new best train loss = 0.008623\n",
      "Generation  51/400 | Population mean: 0.01063 | Best train loss: 0.00862 | Val loss: 0.00627 | Pop. STD: 0.145947\n",
      "Generation  52/400 | Population mean: 0.01063 | Best train loss: 0.00862 | Val loss: 0.00627 | Pop. STD: 0.145947\n",
      "Generation  53/400 | Population mean: 0.01062 | Best train loss: 0.00862 | Val loss: 0.00627 | Pop. STD: 0.146271\n",
      "Generation  54/400 | Population mean: 0.01062 | Best train loss: 0.00862 | Val loss: 0.00627 | Pop. STD: 0.146763\n",
      "[gen 054] new best train loss = 0.007823\n",
      "Generation  55/400 | Population mean: 0.01058 | Best train loss: 0.00782 | Val loss: 0.00648 | Pop. STD: 0.147481\n",
      "Generation  56/400 | Population mean: 0.01048 | Best train loss: 0.00782 | Val loss: 0.00648 | Pop. STD: 0.147349\n",
      "Generation  57/400 | Population mean: 0.01041 | Best train loss: 0.00782 | Val loss: 0.00648 | Pop. STD: 0.147401\n",
      "[gen 057] new best train loss = 0.007801\n",
      "Generation  58/400 | Population mean: 0.01041 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.147145\n",
      "Generation  59/400 | Population mean: 0.01020 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.147894\n",
      "Generation  60/400 | Population mean: 0.00994 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146867\n",
      "Generation  61/400 | Population mean: 0.00991 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146700\n",
      "Generation  62/400 | Population mean: 0.00945 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146491\n",
      "Generation  63/400 | Population mean: 0.00945 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146491\n",
      "Generation  64/400 | Population mean: 0.00945 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146491\n",
      "Generation  65/400 | Population mean: 0.00929 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146544\n",
      "Generation  66/400 | Population mean: 0.00922 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146111\n",
      "Generation  67/400 | Population mean: 0.00914 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146770\n",
      "Generation  68/400 | Population mean: 0.00898 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146293\n",
      "Generation  69/400 | Population mean: 0.00891 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146758\n",
      "Generation  70/400 | Population mean: 0.00888 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.146757\n",
      "Generation  71/400 | Population mean: 0.00885 | Best train loss: 0.00780 | Val loss: 0.01154 | Pop. STD: 0.147128\n",
      "[gen 071] new best train loss = 0.007234\n",
      "Generation  72/400 | Population mean: 0.00877 | Best train loss: 0.00723 | Val loss: 0.00735 | Pop. STD: 0.147344\n",
      "Generation  73/400 | Population mean: 0.00877 | Best train loss: 0.00723 | Val loss: 0.00735 | Pop. STD: 0.147344\n",
      "Generation  74/400 | Population mean: 0.00858 | Best train loss: 0.00723 | Val loss: 0.00735 | Pop. STD: 0.147673\n",
      "Generation  75/400 | Population mean: 0.00845 | Best train loss: 0.00723 | Val loss: 0.00735 | Pop. STD: 0.147738\n",
      "[gen 075] new best train loss = 0.007111\n",
      "Generation  76/400 | Population mean: 0.00836 | Best train loss: 0.00711 | Val loss: 0.00528 | Pop. STD: 0.148865\n",
      "[gen 076] new best train loss = 0.006248\n",
      "Generation  77/400 | Population mean: 0.00822 | Best train loss: 0.00625 | Val loss: 0.00506 | Pop. STD: 0.149698\n",
      "Generation  78/400 | Population mean: 0.00814 | Best train loss: 0.00625 | Val loss: 0.00506 | Pop. STD: 0.150250\n",
      "Generation  79/400 | Population mean: 0.00813 | Best train loss: 0.00625 | Val loss: 0.00506 | Pop. STD: 0.150216\n",
      "Generation  80/400 | Population mean: 0.00801 | Best train loss: 0.00625 | Val loss: 0.00506 | Pop. STD: 0.149858\n",
      "Generation  81/400 | Population mean: 0.00801 | Best train loss: 0.00625 | Val loss: 0.00506 | Pop. STD: 0.149858\n",
      "Generation  82/400 | Population mean: 0.00792 | Best train loss: 0.00625 | Val loss: 0.00506 | Pop. STD: 0.150384\n",
      "[gen 082] new best train loss = 0.005768\n",
      "Generation  83/400 | Population mean: 0.00780 | Best train loss: 0.00577 | Val loss: 0.00838 | Pop. STD: 0.150556\n",
      "Generation  84/400 | Population mean: 0.00780 | Best train loss: 0.00577 | Val loss: 0.00838 | Pop. STD: 0.150556\n",
      "[gen 084] new best train loss = 0.005758\n",
      "Generation  85/400 | Population mean: 0.00732 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.150129\n",
      "Generation  86/400 | Population mean: 0.00727 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.148968\n",
      "Generation  87/400 | Population mean: 0.00721 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.149420\n",
      "Generation  88/400 | Population mean: 0.00718 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.149150\n",
      "Generation  89/400 | Population mean: 0.00712 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.149038\n",
      "Generation  90/400 | Population mean: 0.00699 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.149295\n",
      "Generation  91/400 | Population mean: 0.00687 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.149125\n",
      "Generation  92/400 | Population mean: 0.00676 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.148964\n",
      "Generation  93/400 | Population mean: 0.00676 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.148964\n",
      "Generation  94/400 | Population mean: 0.00672 | Best train loss: 0.00576 | Val loss: 0.00531 | Pop. STD: 0.149228\n",
      "[gen 094] new best train loss = 0.005055\n",
      "Generation  95/400 | Population mean: 0.00658 | Best train loss: 0.00506 | Val loss: 0.00366 | Pop. STD: 0.149193\n",
      "[gen 095] new best train loss = 0.004764\n",
      "Generation  96/400 | Population mean: 0.00647 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.149534\n",
      "Generation  97/400 | Population mean: 0.00645 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.149616\n",
      "Generation  98/400 | Population mean: 0.00640 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.149828\n",
      "Generation  99/400 | Population mean: 0.00630 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.150181\n",
      "Generation 100/400 | Population mean: 0.00612 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.149771\n",
      "Generation 101/400 | Population mean: 0.00601 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.149877\n",
      "Generation 102/400 | Population mean: 0.00601 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.149877\n",
      "Generation 103/400 | Population mean: 0.00600 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.150080\n",
      "Generation 104/400 | Population mean: 0.00588 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.150186\n",
      "Generation 105/400 | Population mean: 0.00578 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.150257\n",
      "Generation 106/400 | Population mean: 0.00574 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.150552\n",
      "Generation 107/400 | Population mean: 0.00567 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.150506\n",
      "Generation 108/400 | Population mean: 0.00567 | Best train loss: 0.00476 | Val loss: 0.00887 | Pop. STD: 0.150506\n",
      "[gen 108] new best train loss = 0.004404\n",
      "Generation 109/400 | Population mean: 0.00556 | Best train loss: 0.00440 | Val loss: 0.00535 | Pop. STD: 0.150708\n",
      "Generation 110/400 | Population mean: 0.00556 | Best train loss: 0.00440 | Val loss: 0.00535 | Pop. STD: 0.150708\n",
      "Generation 111/400 | Population mean: 0.00549 | Best train loss: 0.00440 | Val loss: 0.00535 | Pop. STD: 0.150850\n",
      "Generation 112/400 | Population mean: 0.00545 | Best train loss: 0.00440 | Val loss: 0.00535 | Pop. STD: 0.150853\n",
      "Generation 113/400 | Population mean: 0.00539 | Best train loss: 0.00440 | Val loss: 0.00535 | Pop. STD: 0.151075\n",
      "Generation 114/400 | Population mean: 0.00535 | Best train loss: 0.00440 | Val loss: 0.00535 | Pop. STD: 0.151215\n",
      "[gen 114] new best train loss = 0.004232\n",
      "Generation 115/400 | Population mean: 0.00527 | Best train loss: 0.00423 | Val loss: 0.00476 | Pop. STD: 0.151221\n",
      "Generation 116/400 | Population mean: 0.00514 | Best train loss: 0.00423 | Val loss: 0.00476 | Pop. STD: 0.151372\n",
      "[gen 116] new best train loss = 0.004141\n",
      "Generation 117/400 | Population mean: 0.00504 | Best train loss: 0.00414 | Val loss: 0.00423 | Pop. STD: 0.151287\n",
      "Generation 118/400 | Population mean: 0.00497 | Best train loss: 0.00414 | Val loss: 0.00423 | Pop. STD: 0.150687\n",
      "[gen 118] new best train loss = 0.003992\n",
      "Generation 119/400 | Population mean: 0.00478 | Best train loss: 0.00399 | Val loss: 0.00420 | Pop. STD: 0.150901\n",
      "Generation 120/400 | Population mean: 0.00472 | Best train loss: 0.00399 | Val loss: 0.00420 | Pop. STD: 0.151320\n",
      "Generation 121/400 | Population mean: 0.00463 | Best train loss: 0.00399 | Val loss: 0.00420 | Pop. STD: 0.151379\n",
      "[gen 121] new best train loss = 0.003775\n",
      "Generation 122/400 | Population mean: 0.00456 | Best train loss: 0.00378 | Val loss: 0.00424 | Pop. STD: 0.150996\n",
      "Generation 123/400 | Population mean: 0.00451 | Best train loss: 0.00378 | Val loss: 0.00424 | Pop. STD: 0.150843\n",
      "Generation 124/400 | Population mean: 0.00446 | Best train loss: 0.00378 | Val loss: 0.00424 | Pop. STD: 0.151174\n",
      "Generation 125/400 | Population mean: 0.00443 | Best train loss: 0.00378 | Val loss: 0.00424 | Pop. STD: 0.151314\n",
      "[gen 125] new best train loss = 0.003741\n",
      "[gen 125] new best train loss = 0.003709\n",
      "Generation 126/400 | Population mean: 0.00438 | Best train loss: 0.00371 | Val loss: 0.00423 | Pop. STD: 0.151217\n",
      "Generation 127/400 | Population mean: 0.00434 | Best train loss: 0.00371 | Val loss: 0.00423 | Pop. STD: 0.151272\n",
      "Generation 128/400 | Population mean: 0.00420 | Best train loss: 0.00371 | Val loss: 0.00423 | Pop. STD: 0.151420\n",
      "[gen 128] new best train loss = 0.003557\n",
      "Generation 129/400 | Population mean: 0.00407 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151139\n",
      "Generation 130/400 | Population mean: 0.00405 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151252\n",
      "Generation 131/400 | Population mean: 0.00399 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151376\n",
      "Generation 132/400 | Population mean: 0.00390 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151555\n",
      "Generation 133/400 | Population mean: 0.00390 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151555\n",
      "Generation 134/400 | Population mean: 0.00388 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151482\n",
      "Generation 135/400 | Population mean: 0.00386 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151412\n",
      "Generation 136/400 | Population mean: 0.00383 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151159\n",
      "Generation 137/400 | Population mean: 0.00383 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151159\n",
      "Generation 138/400 | Population mean: 0.00380 | Best train loss: 0.00356 | Val loss: 0.00354 | Pop. STD: 0.151388\n",
      "[gen 138] new best train loss = 0.003385\n",
      "[gen 138] new best train loss = 0.003147\n",
      "Generation 139/400 | Population mean: 0.00375 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151634\n",
      "Generation 140/400 | Population mean: 0.00372 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151397\n",
      "Generation 141/400 | Population mean: 0.00370 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151480\n",
      "Generation 142/400 | Population mean: 0.00365 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151413\n",
      "Generation 143/400 | Population mean: 0.00362 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151810\n",
      "Generation 144/400 | Population mean: 0.00356 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151796\n",
      "Generation 145/400 | Population mean: 0.00354 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151853\n",
      "Generation 146/400 | Population mean: 0.00351 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.151889\n",
      "Generation 147/400 | Population mean: 0.00348 | Best train loss: 0.00315 | Val loss: 0.00460 | Pop. STD: 0.152227\n",
      "[gen 147] new best train loss = 0.003080\n",
      "Generation 148/400 | Population mean: 0.00344 | Best train loss: 0.00308 | Val loss: 0.00434 | Pop. STD: 0.152240\n",
      "Generation 149/400 | Population mean: 0.00342 | Best train loss: 0.00308 | Val loss: 0.00434 | Pop. STD: 0.152121\n",
      "Generation 150/400 | Population mean: 0.00339 | Best train loss: 0.00308 | Val loss: 0.00434 | Pop. STD: 0.152231\n",
      "Generation 151/400 | Population mean: 0.00337 | Best train loss: 0.00308 | Val loss: 0.00434 | Pop. STD: 0.152106\n",
      "[gen 151] new best train loss = 0.003029\n",
      "Generation 152/400 | Population mean: 0.00333 | Best train loss: 0.00303 | Val loss: 0.00319 | Pop. STD: 0.152231\n",
      "Generation 153/400 | Population mean: 0.00331 | Best train loss: 0.00303 | Val loss: 0.00319 | Pop. STD: 0.152246\n",
      "Generation 154/400 | Population mean: 0.00331 | Best train loss: 0.00303 | Val loss: 0.00319 | Pop. STD: 0.152246\n",
      "[gen 154] new best train loss = 0.002976\n",
      "Generation 155/400 | Population mean: 0.00326 | Best train loss: 0.00298 | Val loss: 0.00341 | Pop. STD: 0.152162\n",
      "Generation 156/400 | Population mean: 0.00325 | Best train loss: 0.00298 | Val loss: 0.00341 | Pop. STD: 0.152301\n",
      "Generation 157/400 | Population mean: 0.00324 | Best train loss: 0.00298 | Val loss: 0.00341 | Pop. STD: 0.152237\n",
      "[gen 157] new best train loss = 0.002855\n",
      "Generation 158/400 | Population mean: 0.00321 | Best train loss: 0.00285 | Val loss: 0.00243 | Pop. STD: 0.152235\n",
      "Generation 159/400 | Population mean: 0.00320 | Best train loss: 0.00285 | Val loss: 0.00243 | Pop. STD: 0.152075\n",
      "Generation 160/400 | Population mean: 0.00317 | Best train loss: 0.00285 | Val loss: 0.00243 | Pop. STD: 0.152150\n",
      "[gen 160] new best train loss = 0.002820\n",
      "[gen 160] new best train loss = 0.002747\n",
      "Generation 161/400 | Population mean: 0.00312 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.152078\n",
      "Generation 162/400 | Population mean: 0.00310 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.152121\n",
      "Generation 163/400 | Population mean: 0.00305 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.151990\n",
      "Generation 164/400 | Population mean: 0.00301 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.151988\n",
      "Generation 165/400 | Population mean: 0.00301 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.151988\n",
      "Generation 166/400 | Population mean: 0.00300 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.151869\n",
      "Generation 167/400 | Population mean: 0.00299 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.151791\n",
      "Generation 168/400 | Population mean: 0.00297 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.151764\n",
      "Generation 169/400 | Population mean: 0.00296 | Best train loss: 0.00275 | Val loss: 0.00448 | Pop. STD: 0.151683\n",
      "[gen 169] new best train loss = 0.002702\n",
      "Generation 170/400 | Population mean: 0.00293 | Best train loss: 0.00270 | Val loss: 0.00371 | Pop. STD: 0.151601\n",
      "Generation 171/400 | Population mean: 0.00291 | Best train loss: 0.00270 | Val loss: 0.00371 | Pop. STD: 0.151569\n",
      "Generation 172/400 | Population mean: 0.00290 | Best train loss: 0.00270 | Val loss: 0.00371 | Pop. STD: 0.151628\n",
      "[gen 172] new best train loss = 0.002695\n",
      "Generation 173/400 | Population mean: 0.00287 | Best train loss: 0.00269 | Val loss: 0.00332 | Pop. STD: 0.151582\n",
      "[gen 173] new best train loss = 0.002503\n",
      "Generation 174/400 | Population mean: 0.00282 | Best train loss: 0.00250 | Val loss: 0.00260 | Pop. STD: 0.151928\n",
      "Generation 175/400 | Population mean: 0.00281 | Best train loss: 0.00250 | Val loss: 0.00260 | Pop. STD: 0.151902\n",
      "Generation 176/400 | Population mean: 0.00280 | Best train loss: 0.00250 | Val loss: 0.00260 | Pop. STD: 0.151878\n",
      "Generation 177/400 | Population mean: 0.00278 | Best train loss: 0.00250 | Val loss: 0.00260 | Pop. STD: 0.151851\n",
      "Generation 178/400 | Population mean: 0.00276 | Best train loss: 0.00250 | Val loss: 0.00260 | Pop. STD: 0.152043\n",
      "Generation 179/400 | Population mean: 0.00275 | Best train loss: 0.00250 | Val loss: 0.00260 | Pop. STD: 0.152113\n",
      "[gen 179] new best train loss = 0.002410\n",
      "Generation 180/400 | Population mean: 0.00271 | Best train loss: 0.00241 | Val loss: 0.00297 | Pop. STD: 0.152302\n",
      "[gen 180] new best train loss = 0.002340\n",
      "Generation 181/400 | Population mean: 0.00267 | Best train loss: 0.00234 | Val loss: 0.00291 | Pop. STD: 0.152230\n",
      "Generation 182/400 | Population mean: 0.00265 | Best train loss: 0.00234 | Val loss: 0.00291 | Pop. STD: 0.152316\n",
      "Generation 183/400 | Population mean: 0.00261 | Best train loss: 0.00234 | Val loss: 0.00291 | Pop. STD: 0.152428\n",
      "Generation 184/400 | Population mean: 0.00259 | Best train loss: 0.00234 | Val loss: 0.00291 | Pop. STD: 0.152316\n",
      "[gen 184] new best train loss = 0.002332\n",
      "Generation 185/400 | Population mean: 0.00256 | Best train loss: 0.00233 | Val loss: 0.00353 | Pop. STD: 0.152341\n",
      "Generation 186/400 | Population mean: 0.00255 | Best train loss: 0.00233 | Val loss: 0.00353 | Pop. STD: 0.152530\n",
      "Generation 187/400 | Population mean: 0.00254 | Best train loss: 0.00233 | Val loss: 0.00353 | Pop. STD: 0.152708\n",
      "Generation 188/400 | Population mean: 0.00252 | Best train loss: 0.00233 | Val loss: 0.00353 | Pop. STD: 0.152663\n",
      "[gen 188] new best train loss = 0.002230\n",
      "Generation 189/400 | Population mean: 0.00249 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152692\n",
      "Generation 190/400 | Population mean: 0.00246 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152731\n",
      "Generation 191/400 | Population mean: 0.00245 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152839\n",
      "Generation 192/400 | Population mean: 0.00242 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152776\n",
      "Generation 193/400 | Population mean: 0.00238 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152597\n",
      "Generation 194/400 | Population mean: 0.00238 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152504\n",
      "Generation 195/400 | Population mean: 0.00236 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152447\n",
      "Generation 196/400 | Population mean: 0.00235 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152452\n",
      "Generation 197/400 | Population mean: 0.00234 | Best train loss: 0.00223 | Val loss: 0.00286 | Pop. STD: 0.152496\n",
      "[gen 197] new best train loss = 0.002203\n",
      "Generation 198/400 | Population mean: 0.00233 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152647\n",
      "Generation 199/400 | Population mean: 0.00232 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152377\n",
      "Generation 200/400 | Population mean: 0.00231 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152233\n",
      "Generation 201/400 | Population mean: 0.00230 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152249\n",
      "Generation 202/400 | Population mean: 0.00230 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152249\n",
      "Generation 203/400 | Population mean: 0.00228 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152260\n",
      "Generation 204/400 | Population mean: 0.00228 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152321\n",
      "Generation 205/400 | Population mean: 0.00228 | Best train loss: 0.00220 | Val loss: 0.00300 | Pop. STD: 0.152321\n",
      "[gen 205] new best train loss = 0.002168\n",
      "Generation 206/400 | Population mean: 0.00227 | Best train loss: 0.00217 | Val loss: 0.00320 | Pop. STD: 0.152368\n",
      "Generation 207/400 | Population mean: 0.00227 | Best train loss: 0.00217 | Val loss: 0.00320 | Pop. STD: 0.152368\n",
      "[gen 207] new best train loss = 0.002123\n",
      "Generation 208/400 | Population mean: 0.00226 | Best train loss: 0.00212 | Val loss: 0.00341 | Pop. STD: 0.152378\n",
      "Generation 209/400 | Population mean: 0.00224 | Best train loss: 0.00212 | Val loss: 0.00341 | Pop. STD: 0.152268\n",
      "Generation 210/400 | Population mean: 0.00223 | Best train loss: 0.00212 | Val loss: 0.00341 | Pop. STD: 0.152414\n",
      "[gen 210] new best train loss = 0.002122\n",
      "Generation 211/400 | Population mean: 0.00220 | Best train loss: 0.00212 | Val loss: 0.00315 | Pop. STD: 0.152627\n",
      "[gen 211] new best train loss = 0.002070\n",
      "[gen 211] new best train loss = 0.002031\n",
      "Generation 212/400 | Population mean: 0.00218 | Best train loss: 0.00203 | Val loss: 0.00230 | Pop. STD: 0.152646\n",
      "Generation 213/400 | Population mean: 0.00218 | Best train loss: 0.00203 | Val loss: 0.00230 | Pop. STD: 0.152831\n",
      "Generation 214/400 | Population mean: 0.00215 | Best train loss: 0.00203 | Val loss: 0.00230 | Pop. STD: 0.152906\n",
      "Generation 215/400 | Population mean: 0.00215 | Best train loss: 0.00203 | Val loss: 0.00230 | Pop. STD: 0.153157\n",
      "[gen 215] new best train loss = 0.002004\n",
      "Generation 216/400 | Population mean: 0.00214 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153232\n",
      "Generation 217/400 | Population mean: 0.00214 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153083\n",
      "Generation 218/400 | Population mean: 0.00212 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153100\n",
      "Generation 219/400 | Population mean: 0.00212 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153167\n",
      "Generation 220/400 | Population mean: 0.00210 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153065\n",
      "Generation 221/400 | Population mean: 0.00209 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153129\n",
      "Generation 222/400 | Population mean: 0.00209 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153133\n",
      "Generation 223/400 | Population mean: 0.00209 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153133\n",
      "Generation 224/400 | Population mean: 0.00208 | Best train loss: 0.00200 | Val loss: 0.00219 | Pop. STD: 0.153271\n",
      "[gen 224] new best train loss = 0.001880\n",
      "Generation 225/400 | Population mean: 0.00207 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153285\n",
      "Generation 226/400 | Population mean: 0.00205 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153320\n",
      "Generation 227/400 | Population mean: 0.00204 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153472\n",
      "Generation 228/400 | Population mean: 0.00203 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153442\n",
      "Generation 229/400 | Population mean: 0.00202 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153454\n",
      "Generation 230/400 | Population mean: 0.00200 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153557\n",
      "Generation 231/400 | Population mean: 0.00200 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153555\n",
      "Generation 232/400 | Population mean: 0.00199 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153563\n",
      "Generation 233/400 | Population mean: 0.00199 | Best train loss: 0.00188 | Val loss: 0.00256 | Pop. STD: 0.153563\n",
      "[gen 233] new best train loss = 0.001861\n",
      "Generation 234/400 | Population mean: 0.00197 | Best train loss: 0.00186 | Val loss: 0.00192 | Pop. STD: 0.153389\n",
      "Generation 235/400 | Population mean: 0.00197 | Best train loss: 0.00186 | Val loss: 0.00192 | Pop. STD: 0.153389\n",
      "Generation 236/400 | Population mean: 0.00196 | Best train loss: 0.00186 | Val loss: 0.00192 | Pop. STD: 0.153633\n",
      "[gen 236] new best train loss = 0.001817\n",
      "Generation 237/400 | Population mean: 0.00195 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.153936\n",
      "Generation 238/400 | Population mean: 0.00194 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.153881\n",
      "Generation 239/400 | Population mean: 0.00194 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.153800\n",
      "Generation 240/400 | Population mean: 0.00194 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.153800\n",
      "Generation 241/400 | Population mean: 0.00193 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.154046\n",
      "Generation 242/400 | Population mean: 0.00193 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.154044\n",
      "Generation 243/400 | Population mean: 0.00191 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.154106\n",
      "Generation 244/400 | Population mean: 0.00191 | Best train loss: 0.00182 | Val loss: 0.00228 | Pop. STD: 0.154106\n",
      "[gen 244] new best train loss = 0.001761\n",
      "Generation 245/400 | Population mean: 0.00190 | Best train loss: 0.00176 | Val loss: 0.00249 | Pop. STD: 0.154217\n",
      "Generation 246/400 | Population mean: 0.00190 | Best train loss: 0.00176 | Val loss: 0.00249 | Pop. STD: 0.154306\n",
      "Generation 247/400 | Population mean: 0.00189 | Best train loss: 0.00176 | Val loss: 0.00249 | Pop. STD: 0.154442\n",
      "[gen 247] new best train loss = 0.001733\n",
      "Generation 248/400 | Population mean: 0.00187 | Best train loss: 0.00173 | Val loss: 0.00216 | Pop. STD: 0.154257\n",
      "Generation 249/400 | Population mean: 0.00186 | Best train loss: 0.00173 | Val loss: 0.00216 | Pop. STD: 0.154180\n",
      "[gen 249] new best train loss = 0.001695\n",
      "Generation 250/400 | Population mean: 0.00185 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154197\n",
      "Generation 251/400 | Population mean: 0.00183 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154029\n",
      "Generation 252/400 | Population mean: 0.00182 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154140\n",
      "Generation 253/400 | Population mean: 0.00182 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154140\n",
      "Generation 254/400 | Population mean: 0.00181 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154161\n",
      "Generation 255/400 | Population mean: 0.00180 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154098\n",
      "Generation 256/400 | Population mean: 0.00180 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154093\n",
      "Generation 257/400 | Population mean: 0.00178 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154128\n",
      "Generation 258/400 | Population mean: 0.00178 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154099\n",
      "Generation 259/400 | Population mean: 0.00177 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154067\n",
      "Generation 260/400 | Population mean: 0.00177 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154035\n",
      "Generation 261/400 | Population mean: 0.00176 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154061\n",
      "Generation 262/400 | Population mean: 0.00176 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154062\n",
      "Generation 263/400 | Population mean: 0.00176 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154062\n",
      "Generation 264/400 | Population mean: 0.00176 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154051\n",
      "Generation 265/400 | Population mean: 0.00176 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.154051\n",
      "Generation 266/400 | Population mean: 0.00175 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.153982\n",
      "Generation 267/400 | Population mean: 0.00174 | Best train loss: 0.00169 | Val loss: 0.00240 | Pop. STD: 0.153960\n",
      "[gen 267] new best train loss = 0.001669\n",
      "Generation 268/400 | Population mean: 0.00173 | Best train loss: 0.00167 | Val loss: 0.00206 | Pop. STD: 0.153860\n",
      "[gen 268] new best train loss = 0.001668\n",
      "Generation 269/400 | Population mean: 0.00172 | Best train loss: 0.00167 | Val loss: 0.00202 | Pop. STD: 0.153941\n",
      "Generation 270/400 | Population mean: 0.00172 | Best train loss: 0.00167 | Val loss: 0.00202 | Pop. STD: 0.153941\n",
      "[gen 270] new best train loss = 0.001655\n",
      "Generation 271/400 | Population mean: 0.00171 | Best train loss: 0.00165 | Val loss: 0.00219 | Pop. STD: 0.153955\n",
      "Generation 272/400 | Population mean: 0.00170 | Best train loss: 0.00165 | Val loss: 0.00219 | Pop. STD: 0.153923\n",
      "Generation 273/400 | Population mean: 0.00170 | Best train loss: 0.00165 | Val loss: 0.00219 | Pop. STD: 0.154124\n",
      "Generation 274/400 | Population mean: 0.00170 | Best train loss: 0.00165 | Val loss: 0.00219 | Pop. STD: 0.154085\n",
      "Generation 275/400 | Population mean: 0.00170 | Best train loss: 0.00165 | Val loss: 0.00219 | Pop. STD: 0.154010\n",
      "[gen 275] new best train loss = 0.001653\n",
      "Generation 276/400 | Population mean: 0.00169 | Best train loss: 0.00165 | Val loss: 0.00213 | Pop. STD: 0.154034\n",
      "Generation 277/400 | Population mean: 0.00169 | Best train loss: 0.00165 | Val loss: 0.00213 | Pop. STD: 0.154117\n",
      "[gen 277] new best train loss = 0.001653\n",
      "[gen 277] new best train loss = 0.001634\n",
      "Generation 278/400 | Population mean: 0.00169 | Best train loss: 0.00163 | Val loss: 0.00201 | Pop. STD: 0.154197\n",
      "Generation 279/400 | Population mean: 0.00169 | Best train loss: 0.00163 | Val loss: 0.00201 | Pop. STD: 0.154197\n",
      "[gen 279] new best train loss = 0.001620\n",
      "Generation 280/400 | Population mean: 0.00168 | Best train loss: 0.00162 | Val loss: 0.00212 | Pop. STD: 0.154054\n",
      "[gen 280] new best train loss = 0.001616\n",
      "Generation 281/400 | Population mean: 0.00167 | Best train loss: 0.00162 | Val loss: 0.00215 | Pop. STD: 0.154029\n",
      "Generation 282/400 | Population mean: 0.00167 | Best train loss: 0.00162 | Val loss: 0.00215 | Pop. STD: 0.154070\n",
      "Generation 283/400 | Population mean: 0.00167 | Best train loss: 0.00162 | Val loss: 0.00215 | Pop. STD: 0.154152\n",
      "[gen 283] new best train loss = 0.001584\n",
      "Generation 284/400 | Population mean: 0.00166 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154126\n",
      "Generation 285/400 | Population mean: 0.00165 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154109\n",
      "Generation 286/400 | Population mean: 0.00165 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154183\n",
      "Generation 287/400 | Population mean: 0.00165 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154181\n",
      "Generation 288/400 | Population mean: 0.00165 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154205\n",
      "Generation 289/400 | Population mean: 0.00165 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154247\n",
      "Generation 290/400 | Population mean: 0.00164 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154233\n",
      "Generation 291/400 | Population mean: 0.00164 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154233\n",
      "Generation 292/400 | Population mean: 0.00164 | Best train loss: 0.00158 | Val loss: 0.00200 | Pop. STD: 0.154233\n",
      "[gen 292] new best train loss = 0.001582\n",
      "[gen 292] new best train loss = 0.001581\n",
      "Generation 293/400 | Population mean: 0.00163 | Best train loss: 0.00158 | Val loss: 0.00199 | Pop. STD: 0.154169\n",
      "Generation 294/400 | Population mean: 0.00163 | Best train loss: 0.00158 | Val loss: 0.00199 | Pop. STD: 0.154169\n",
      "Generation 295/400 | Population mean: 0.00163 | Best train loss: 0.00158 | Val loss: 0.00199 | Pop. STD: 0.154129\n",
      "[gen 295] new best train loss = 0.001577\n",
      "Generation 296/400 | Population mean: 0.00163 | Best train loss: 0.00158 | Val loss: 0.00187 | Pop. STD: 0.154136\n",
      "[gen 296] new best train loss = 0.001549\n",
      "Generation 297/400 | Population mean: 0.00162 | Best train loss: 0.00155 | Val loss: 0.00192 | Pop. STD: 0.154165\n",
      "Generation 298/400 | Population mean: 0.00162 | Best train loss: 0.00155 | Val loss: 0.00192 | Pop. STD: 0.154152\n",
      "[gen 298] new best train loss = 0.001537\n",
      "Generation 299/400 | Population mean: 0.00161 | Best train loss: 0.00154 | Val loss: 0.00184 | Pop. STD: 0.154226\n",
      "Generation 300/400 | Population mean: 0.00160 | Best train loss: 0.00154 | Val loss: 0.00184 | Pop. STD: 0.154193\n",
      "Generation 301/400 | Population mean: 0.00160 | Best train loss: 0.00154 | Val loss: 0.00184 | Pop. STD: 0.154193\n",
      "Generation 302/400 | Population mean: 0.00160 | Best train loss: 0.00154 | Val loss: 0.00184 | Pop. STD: 0.154206\n",
      "Generation 303/400 | Population mean: 0.00160 | Best train loss: 0.00154 | Val loss: 0.00184 | Pop. STD: 0.154132\n",
      "Generation 304/400 | Population mean: 0.00160 | Best train loss: 0.00154 | Val loss: 0.00184 | Pop. STD: 0.154200\n",
      "Generation 305/400 | Population mean: 0.00160 | Best train loss: 0.00154 | Val loss: 0.00184 | Pop. STD: 0.154187\n",
      "[gen 305] new best train loss = 0.001513\n",
      "Generation 306/400 | Population mean: 0.00159 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154152\n",
      "Generation 307/400 | Population mean: 0.00159 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154133\n",
      "Generation 308/400 | Population mean: 0.00158 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154111\n",
      "Generation 309/400 | Population mean: 0.00158 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154035\n",
      "Generation 310/400 | Population mean: 0.00158 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154037\n",
      "Generation 311/400 | Population mean: 0.00157 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153989\n",
      "Generation 312/400 | Population mean: 0.00156 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153961\n",
      "Generation 313/400 | Population mean: 0.00156 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153941\n",
      "Generation 314/400 | Population mean: 0.00156 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153940\n",
      "Generation 315/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153918\n",
      "Generation 316/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153947\n",
      "Generation 317/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153947\n",
      "Generation 318/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153938\n",
      "Generation 319/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153938\n",
      "Generation 320/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153938\n",
      "Generation 321/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153938\n",
      "Generation 322/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153951\n",
      "Generation 323/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153951\n",
      "Generation 324/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.153983\n",
      "Generation 325/400 | Population mean: 0.00155 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154001\n",
      "Generation 326/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154019\n",
      "Generation 327/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154019\n",
      "Generation 328/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154019\n",
      "Generation 329/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154014\n",
      "Generation 330/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154008\n",
      "Generation 331/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154018\n",
      "Generation 332/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00192 | Pop. STD: 0.154041\n",
      "[gen 332] new best train loss = 0.001509\n",
      "Generation 333/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00196 | Pop. STD: 0.154106\n",
      "Generation 334/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00196 | Pop. STD: 0.154189\n",
      "Generation 335/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00196 | Pop. STD: 0.154189\n",
      "Generation 336/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00196 | Pop. STD: 0.154208\n",
      "Generation 337/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00196 | Pop. STD: 0.154208\n",
      "Generation 338/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00196 | Pop. STD: 0.154150\n",
      "Generation 339/400 | Population mean: 0.00154 | Best train loss: 0.00151 | Val loss: 0.00196 | Pop. STD: 0.154132\n",
      "[gen 339] new best train loss = 0.001494\n",
      "Generation 340/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154119\n",
      "Generation 341/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154119\n",
      "[gen 341] new best train loss = 0.001491\n",
      "Generation 342/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00193 | Pop. STD: 0.154120\n",
      "Generation 343/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00193 | Pop. STD: 0.154138\n",
      "Generation 344/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00193 | Pop. STD: 0.154120\n",
      "Generation 345/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00193 | Pop. STD: 0.154105\n",
      "Generation 346/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00193 | Pop. STD: 0.154100\n",
      "Generation 347/400 | Population mean: 0.00153 | Best train loss: 0.00149 | Val loss: 0.00193 | Pop. STD: 0.154100\n",
      "[gen 347] new best train loss = 0.001487\n",
      "Generation 348/400 | Population mean: 0.00152 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154124\n",
      "Generation 349/400 | Population mean: 0.00152 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154106\n",
      "Generation 350/400 | Population mean: 0.00152 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154129\n",
      "Generation 351/400 | Population mean: 0.00152 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154149\n",
      "Generation 352/400 | Population mean: 0.00152 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154154\n",
      "Generation 353/400 | Population mean: 0.00152 | Best train loss: 0.00149 | Val loss: 0.00203 | Pop. STD: 0.154098\n",
      "[gen 353] new best train loss = 0.001473\n",
      "Generation 354/400 | Population mean: 0.00151 | Best train loss: 0.00147 | Val loss: 0.00197 | Pop. STD: 0.154132\n",
      "Generation 355/400 | Population mean: 0.00151 | Best train loss: 0.00147 | Val loss: 0.00197 | Pop. STD: 0.154132\n",
      "Generation 356/400 | Population mean: 0.00151 | Best train loss: 0.00147 | Val loss: 0.00197 | Pop. STD: 0.154144\n",
      "Generation 357/400 | Population mean: 0.00151 | Best train loss: 0.00147 | Val loss: 0.00197 | Pop. STD: 0.154153\n",
      "Generation 358/400 | Population mean: 0.00151 | Best train loss: 0.00147 | Val loss: 0.00197 | Pop. STD: 0.154153\n",
      "Generation 359/400 | Population mean: 0.00151 | Best train loss: 0.00147 | Val loss: 0.00197 | Pop. STD: 0.154214\n",
      "[gen 359] new best train loss = 0.001459\n",
      "Generation 360/400 | Population mean: 0.00151 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154187\n",
      "Generation 361/400 | Population mean: 0.00151 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154187\n",
      "Generation 362/400 | Population mean: 0.00151 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154187\n",
      "Generation 363/400 | Population mean: 0.00151 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154176\n",
      "Generation 364/400 | Population mean: 0.00151 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154176\n",
      "Generation 365/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154170\n",
      "Generation 366/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154154\n",
      "Generation 367/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154154\n",
      "Generation 368/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154154\n",
      "Generation 369/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154139\n",
      "Generation 370/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154123\n",
      "Generation 371/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154135\n",
      "Generation 372/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154135\n",
      "Generation 373/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154212\n",
      "Generation 374/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154212\n",
      "Generation 375/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154181\n",
      "Generation 376/400 | Population mean: 0.00150 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154181\n",
      "[gen 376] new best train loss = 0.001457\n",
      "Generation 377/400 | Population mean: 0.00149 | Best train loss: 0.00146 | Val loss: 0.00178 | Pop. STD: 0.154182\n",
      "[gen 377] new best train loss = 0.001438\n",
      "Generation 378/400 | Population mean: 0.00149 | Best train loss: 0.00144 | Val loss: 0.00176 | Pop. STD: 0.154203\n",
      "Generation 379/400 | Population mean: 0.00148 | Best train loss: 0.00144 | Val loss: 0.00176 | Pop. STD: 0.154230\n",
      "Generation 380/400 | Population mean: 0.00148 | Best train loss: 0.00144 | Val loss: 0.00176 | Pop. STD: 0.154296\n",
      "Generation 381/400 | Population mean: 0.00148 | Best train loss: 0.00144 | Val loss: 0.00176 | Pop. STD: 0.154307\n",
      "[gen 381] new best train loss = 0.001431\n",
      "Generation 382/400 | Population mean: 0.00147 | Best train loss: 0.00143 | Val loss: 0.00186 | Pop. STD: 0.154326\n",
      "Generation 383/400 | Population mean: 0.00147 | Best train loss: 0.00143 | Val loss: 0.00186 | Pop. STD: 0.154337\n",
      "Generation 384/400 | Population mean: 0.00146 | Best train loss: 0.00143 | Val loss: 0.00186 | Pop. STD: 0.154313\n",
      "Generation 385/400 | Population mean: 0.00146 | Best train loss: 0.00143 | Val loss: 0.00186 | Pop. STD: 0.154323\n",
      "Generation 386/400 | Population mean: 0.00146 | Best train loss: 0.00143 | Val loss: 0.00186 | Pop. STD: 0.154311\n",
      "[gen 386] new best train loss = 0.001407\n",
      "Generation 387/400 | Population mean: 0.00146 | Best train loss: 0.00141 | Val loss: 0.00168 | Pop. STD: 0.154366\n",
      "Generation 388/400 | Population mean: 0.00146 | Best train loss: 0.00141 | Val loss: 0.00168 | Pop. STD: 0.154378\n",
      "Generation 389/400 | Population mean: 0.00146 | Best train loss: 0.00141 | Val loss: 0.00168 | Pop. STD: 0.154415\n",
      "Generation 390/400 | Population mean: 0.00146 | Best train loss: 0.00141 | Val loss: 0.00168 | Pop. STD: 0.154409\n",
      "Generation 391/400 | Population mean: 0.00146 | Best train loss: 0.00141 | Val loss: 0.00168 | Pop. STD: 0.154435\n",
      "Generation 392/400 | Population mean: 0.00145 | Best train loss: 0.00141 | Val loss: 0.00168 | Pop. STD: 0.154495\n",
      "Generation 393/400 | Population mean: 0.00144 | Best train loss: 0.00141 | Val loss: 0.00168 | Pop. STD: 0.154596\n",
      "[gen 393] new best train loss = 0.001398\n",
      "Generation 394/400 | Population mean: 0.00144 | Best train loss: 0.00140 | Val loss: 0.00180 | Pop. STD: 0.154604\n",
      "[gen 394] new best train loss = 0.001398\n",
      "[gen 394] new best train loss = 0.001396\n",
      "Generation 395/400 | Population mean: 0.00143 | Best train loss: 0.00140 | Val loss: 0.00162 | Pop. STD: 0.154581\n",
      "Generation 396/400 | Population mean: 0.00143 | Best train loss: 0.00140 | Val loss: 0.00162 | Pop. STD: 0.154587\n",
      "Generation 397/400 | Population mean: 0.00143 | Best train loss: 0.00140 | Val loss: 0.00162 | Pop. STD: 0.154594\n",
      "Generation 398/400 | Population mean: 0.00143 | Best train loss: 0.00140 | Val loss: 0.00162 | Pop. STD: 0.154608\n",
      "[gen 398] new best train loss = 0.001376\n",
      "Generation 399/400 | Population mean: 0.00142 | Best train loss: 0.00138 | Val loss: 0.00176 | Pop. STD: 0.154601\n",
      "Generation 400/400 | Population mean: 0.00142 | Best train loss: 0.00138 | Val loss: 0.00176 | Pop. STD: 0.154606\n",
      "\n",
      "\n",
      "Final Metrics Summary:\n",
      "========================================\n",
      "DE | Dataset: krokhin.txt | Val R2: 0.892 | MAE: 0.0473 | Eval Calls: 8420 | Time: 273.27s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = []\n",
    "dataset_path = \"data/krokhin.txt\"\n",
    "plt.figure(figsize=(16,9))\n",
    "print(f\"\\n===== Dataset: {dataset_path} =====\")\n",
    "for alg, cfg in zip([\"adam\", \"cmaes\", \"de\"], [ADAM_CONFIG, CMAES_CONFIG, DE_CONFIG]):\n",
    "    print(f\"Running {alg.upper()}...\")\n",
    "    result = run_experiment(alg, dataset_path, cfg)\n",
    "    summarize_results(result)\n",
    "    all_results.append(result)\n",
    "    plot_learning_curves(result[\"history\"], label=alg)\n",
    "\n",
    "plt.title(f\"Learning Curves - {Path(dataset_path).stem}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results/curves_{Path(dataset_path).stem}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Dataset: data/mouse.txt =====\n",
      "Running ADAM...\n",
      "Epoch   1/150 | Train Loss: 0.05592 | Val Loss: 0.02283\n",
      "Epoch   2/150 | Train Loss: 0.01350 | Val Loss: 0.00813\n",
      "Epoch   3/150 | Train Loss: 0.00592 | Val Loss: 0.00579\n",
      "Epoch   4/150 | Train Loss: 0.00452 | Val Loss: 0.00505\n",
      "Epoch   5/150 | Train Loss: 0.00397 | Val Loss: 0.00466\n",
      "Epoch   6/150 | Train Loss: 0.00365 | Val Loss: 0.00428\n",
      "Epoch   7/150 | Train Loss: 0.00338 | Val Loss: 0.00405\n",
      "Epoch   8/150 | Train Loss: 0.00317 | Val Loss: 0.00398\n",
      "Epoch   9/150 | Train Loss: 0.00303 | Val Loss: 0.00374\n",
      "Epoch  10/150 | Train Loss: 0.00290 | Val Loss: 0.00363\n",
      "Epoch  11/150 | Train Loss: 0.00281 | Val Loss: 0.00356\n",
      "Epoch  12/150 | Train Loss: 0.00269 | Val Loss: 0.00344\n",
      "Epoch  13/150 | Train Loss: 0.00263 | Val Loss: 0.00342\n",
      "Epoch  14/150 | Train Loss: 0.00259 | Val Loss: 0.00332\n",
      "Epoch  15/150 | Train Loss: 0.00256 | Val Loss: 0.00362\n",
      "Epoch  16/150 | Train Loss: 0.00251 | Val Loss: 0.00325\n",
      "Epoch  17/150 | Train Loss: 0.00243 | Val Loss: 0.00330\n",
      "Epoch  18/150 | Train Loss: 0.00243 | Val Loss: 0.00318\n",
      "Epoch  19/150 | Train Loss: 0.00236 | Val Loss: 0.00321\n",
      "Epoch  20/150 | Train Loss: 0.00233 | Val Loss: 0.00309\n",
      "Epoch  21/150 | Train Loss: 0.00224 | Val Loss: 0.00306\n",
      "Epoch  22/150 | Train Loss: 0.00223 | Val Loss: 0.00303\n",
      "Epoch  23/150 | Train Loss: 0.00221 | Val Loss: 0.00303\n",
      "Epoch  24/150 | Train Loss: 0.00216 | Val Loss: 0.00292\n",
      "Epoch  25/150 | Train Loss: 0.00211 | Val Loss: 0.00289\n",
      "Epoch  26/150 | Train Loss: 0.00212 | Val Loss: 0.00288\n",
      "Epoch  27/150 | Train Loss: 0.00207 | Val Loss: 0.00298\n",
      "Epoch  28/150 | Train Loss: 0.00203 | Val Loss: 0.00286\n",
      "Epoch  29/150 | Train Loss: 0.00200 | Val Loss: 0.00290\n",
      "Epoch  30/150 | Train Loss: 0.00197 | Val Loss: 0.00277\n",
      "Epoch  31/150 | Train Loss: 0.00193 | Val Loss: 0.00283\n",
      "Epoch  32/150 | Train Loss: 0.00193 | Val Loss: 0.00280\n",
      "Epoch  33/150 | Train Loss: 0.00192 | Val Loss: 0.00273\n",
      "Epoch  34/150 | Train Loss: 0.00184 | Val Loss: 0.00272\n",
      "Epoch  35/150 | Train Loss: 0.00186 | Val Loss: 0.00267\n",
      "Epoch  36/150 | Train Loss: 0.00178 | Val Loss: 0.00268\n",
      "Epoch  37/150 | Train Loss: 0.00184 | Val Loss: 0.00270\n",
      "Epoch  38/150 | Train Loss: 0.00177 | Val Loss: 0.00271\n",
      "Epoch  39/150 | Train Loss: 0.00174 | Val Loss: 0.00278\n",
      "Epoch  40/150 | Train Loss: 0.00170 | Val Loss: 0.00259\n",
      "Epoch  41/150 | Train Loss: 0.00168 | Val Loss: 0.00261\n",
      "Epoch  42/150 | Train Loss: 0.00167 | Val Loss: 0.00261\n",
      "Epoch  43/150 | Train Loss: 0.00166 | Val Loss: 0.00252\n",
      "Epoch  44/150 | Train Loss: 0.00162 | Val Loss: 0.00262\n",
      "Epoch  45/150 | Train Loss: 0.00160 | Val Loss: 0.00257\n",
      "Epoch  46/150 | Train Loss: 0.00161 | Val Loss: 0.00252\n",
      "Epoch  47/150 | Train Loss: 0.00156 | Val Loss: 0.00249\n",
      "Epoch  48/150 | Train Loss: 0.00154 | Val Loss: 0.00250\n",
      "Epoch  49/150 | Train Loss: 0.00154 | Val Loss: 0.00250\n",
      "Epoch  50/150 | Train Loss: 0.00152 | Val Loss: 0.00248\n",
      "Epoch  51/150 | Train Loss: 0.00148 | Val Loss: 0.00247\n",
      "Epoch  52/150 | Train Loss: 0.00149 | Val Loss: 0.00253\n",
      "Epoch  53/150 | Train Loss: 0.00148 | Val Loss: 0.00246\n",
      "Epoch  54/150 | Train Loss: 0.00146 | Val Loss: 0.00242\n",
      "Epoch  55/150 | Train Loss: 0.00148 | Val Loss: 0.00245\n",
      "Epoch  56/150 | Train Loss: 0.00144 | Val Loss: 0.00245\n",
      "Epoch  57/150 | Train Loss: 0.00142 | Val Loss: 0.00242\n",
      "Epoch  58/150 | Train Loss: 0.00140 | Val Loss: 0.00242\n",
      "Epoch  59/150 | Train Loss: 0.00144 | Val Loss: 0.00234\n",
      "Epoch  60/150 | Train Loss: 0.00138 | Val Loss: 0.00242\n",
      "Epoch  61/150 | Train Loss: 0.00136 | Val Loss: 0.00236\n",
      "Epoch  62/150 | Train Loss: 0.00136 | Val Loss: 0.00239\n",
      "Epoch  63/150 | Train Loss: 0.00135 | Val Loss: 0.00240\n",
      "Epoch  64/150 | Train Loss: 0.00135 | Val Loss: 0.00235\n",
      "Epoch  65/150 | Train Loss: 0.00136 | Val Loss: 0.00239\n",
      "Epoch  66/150 | Train Loss: 0.00132 | Val Loss: 0.00234\n",
      "Epoch  67/150 | Train Loss: 0.00132 | Val Loss: 0.00231\n",
      "Epoch  68/150 | Train Loss: 0.00130 | Val Loss: 0.00236\n",
      "Epoch  69/150 | Train Loss: 0.00128 | Val Loss: 0.00233\n",
      "Epoch  70/150 | Train Loss: 0.00130 | Val Loss: 0.00228\n",
      "Epoch  71/150 | Train Loss: 0.00132 | Val Loss: 0.00243\n",
      "Epoch  72/150 | Train Loss: 0.00131 | Val Loss: 0.00236\n",
      "Epoch  73/150 | Train Loss: 0.00125 | Val Loss: 0.00228\n",
      "Epoch  74/150 | Train Loss: 0.00126 | Val Loss: 0.00230\n",
      "Epoch  75/150 | Train Loss: 0.00123 | Val Loss: 0.00239\n",
      "Epoch  76/150 | Train Loss: 0.00127 | Val Loss: 0.00231\n",
      "Epoch  77/150 | Train Loss: 0.00124 | Val Loss: 0.00229\n",
      "Epoch  78/150 | Train Loss: 0.00127 | Val Loss: 0.00232\n",
      "Epoch  79/150 | Train Loss: 0.00121 | Val Loss: 0.00229\n",
      "Epoch  80/150 | Train Loss: 0.00121 | Val Loss: 0.00236\n",
      "Epoch  81/150 | Train Loss: 0.00120 | Val Loss: 0.00238\n",
      "Epoch  82/150 | Train Loss: 0.00123 | Val Loss: 0.00241\n",
      "Epoch  83/150 | Train Loss: 0.00118 | Val Loss: 0.00230\n",
      "Epoch  84/150 | Train Loss: 0.00117 | Val Loss: 0.00231\n",
      "Epoch  85/150 | Train Loss: 0.00119 | Val Loss: 0.00230\n",
      "Epoch  86/150 | Train Loss: 0.00117 | Val Loss: 0.00235\n",
      "Epoch  87/150 | Train Loss: 0.00114 | Val Loss: 0.00228\n",
      "Epoch  88/150 | Train Loss: 0.00116 | Val Loss: 0.00230\n",
      "Epoch  89/150 | Train Loss: 0.00115 | Val Loss: 0.00236\n",
      "Epoch  90/150 | Train Loss: 0.00118 | Val Loss: 0.00238\n",
      "Epoch  91/150 | Train Loss: 0.00114 | Val Loss: 0.00245\n",
      "Epoch  92/150 | Train Loss: 0.00115 | Val Loss: 0.00224\n",
      "Epoch  93/150 | Train Loss: 0.00113 | Val Loss: 0.00232\n",
      "Epoch  94/150 | Train Loss: 0.00113 | Val Loss: 0.00224\n",
      "Epoch  95/150 | Train Loss: 0.00111 | Val Loss: 0.00227\n",
      "Epoch  96/150 | Train Loss: 0.00113 | Val Loss: 0.00233\n",
      "Epoch  97/150 | Train Loss: 0.00112 | Val Loss: 0.00230\n",
      "Epoch  98/150 | Train Loss: 0.00110 | Val Loss: 0.00222\n",
      "Epoch  99/150 | Train Loss: 0.00108 | Val Loss: 0.00224\n",
      "Epoch 100/150 | Train Loss: 0.00111 | Val Loss: 0.00226\n",
      "Epoch 101/150 | Train Loss: 0.00110 | Val Loss: 0.00227\n",
      "Epoch 102/150 | Train Loss: 0.00108 | Val Loss: 0.00223\n",
      "Epoch 103/150 | Train Loss: 0.00108 | Val Loss: 0.00244\n",
      "Epoch 104/150 | Train Loss: 0.00109 | Val Loss: 0.00225\n",
      "Epoch 105/150 | Train Loss: 0.00108 | Val Loss: 0.00231\n",
      "Epoch 106/150 | Train Loss: 0.00106 | Val Loss: 0.00231\n",
      "Epoch 107/150 | Train Loss: 0.00107 | Val Loss: 0.00225\n",
      "Epoch 108/150 | Train Loss: 0.00106 | Val Loss: 0.00229\n",
      "Epoch 109/150 | Train Loss: 0.00105 | Val Loss: 0.00229\n",
      "Epoch 110/150 | Train Loss: 0.00107 | Val Loss: 0.00230\n",
      "Epoch 111/150 | Train Loss: 0.00104 | Val Loss: 0.00242\n",
      "Epoch 112/150 | Train Loss: 0.00104 | Val Loss: 0.00247\n",
      "Epoch 113/150 | Train Loss: 0.00105 | Val Loss: 0.00228\n",
      "Epoch 114/150 | Train Loss: 0.00104 | Val Loss: 0.00225\n",
      "Epoch 115/150 | Train Loss: 0.00103 | Val Loss: 0.00242\n",
      "Epoch 116/150 | Train Loss: 0.00101 | Val Loss: 0.00228\n",
      "Epoch 117/150 | Train Loss: 0.00101 | Val Loss: 0.00243\n",
      "Epoch 118/150 | Train Loss: 0.00102 | Val Loss: 0.00230\n",
      "Epoch 119/150 | Train Loss: 0.00101 | Val Loss: 0.00226\n",
      "Epoch 120/150 | Train Loss: 0.00102 | Val Loss: 0.00231\n",
      "Epoch 121/150 | Train Loss: 0.00101 | Val Loss: 0.00233\n",
      "Epoch 122/150 | Train Loss: 0.00100 | Val Loss: 0.00242\n",
      "Epoch 123/150 | Train Loss: 0.00098 | Val Loss: 0.00230\n",
      "Epoch 124/150 | Train Loss: 0.00100 | Val Loss: 0.00231\n",
      "Epoch 125/150 | Train Loss: 0.00098 | Val Loss: 0.00231\n",
      "Epoch 126/150 | Train Loss: 0.00098 | Val Loss: 0.00241\n",
      "Epoch 127/150 | Train Loss: 0.00098 | Val Loss: 0.00233\n",
      "Epoch 128/150 | Train Loss: 0.00097 | Val Loss: 0.00229\n",
      "Epoch 129/150 | Train Loss: 0.00095 | Val Loss: 0.00235\n",
      "Epoch 130/150 | Train Loss: 0.00096 | Val Loss: 0.00237\n",
      "Epoch 131/150 | Train Loss: 0.00096 | Val Loss: 0.00233\n",
      "Epoch 132/150 | Train Loss: 0.00095 | Val Loss: 0.00237\n",
      "Epoch 133/150 | Train Loss: 0.00096 | Val Loss: 0.00239\n",
      "Epoch 134/150 | Train Loss: 0.00095 | Val Loss: 0.00240\n",
      "Epoch 135/150 | Train Loss: 0.00094 | Val Loss: 0.00231\n",
      "Epoch 136/150 | Train Loss: 0.00093 | Val Loss: 0.00233\n",
      "Epoch 137/150 | Train Loss: 0.00093 | Val Loss: 0.00240\n",
      "Epoch 138/150 | Train Loss: 0.00093 | Val Loss: 0.00240\n",
      "Epoch 139/150 | Train Loss: 0.00093 | Val Loss: 0.00237\n",
      "Epoch 140/150 | Train Loss: 0.00095 | Val Loss: 0.00235\n",
      "Epoch 141/150 | Train Loss: 0.00093 | Val Loss: 0.00240\n",
      "Epoch 142/150 | Train Loss: 0.00093 | Val Loss: 0.00239\n",
      "Epoch 143/150 | Train Loss: 0.00092 | Val Loss: 0.00236\n",
      "Epoch 144/150 | Train Loss: 0.00092 | Val Loss: 0.00242\n",
      "Epoch 145/150 | Train Loss: 0.00096 | Val Loss: 0.00248\n",
      "Epoch 146/150 | Train Loss: 0.00091 | Val Loss: 0.00235\n",
      "Epoch 147/150 | Train Loss: 0.00089 | Val Loss: 0.00236\n",
      "Epoch 148/150 | Train Loss: 0.00091 | Val Loss: 0.00239\n",
      "Epoch 149/150 | Train Loss: 0.00092 | Val Loss: 0.00236\n",
      "Epoch 150/150 | Train Loss: 0.00089 | Val Loss: 0.00238\n",
      "\n",
      "\n",
      "Final Metrics Summary:\n",
      "========================================\n",
      "ADAM | Dataset: mouse.txt | Val R2: 0.908 | MAE: 0.0546 | Eval Calls: 150 | Time: 119.91s\n",
      "Running CMAES...\n",
      "Gen   1/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.49539 | Success: False\n",
      "Gen   2/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.48668 | Success: False\n",
      "Gen   3/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.47442 | Success: False\n",
      "Gen   4/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.45917 | Success: False\n",
      "Gen   5/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.44153 | Success: False\n",
      "Gen   6/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.42202 | Success: False\n",
      "Gen   7/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.40117 | Success: False\n",
      "Gen   8/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.37943 | Success: False\n",
      "Gen   9/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.35722 | Success: False\n",
      "Gen  10/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.33488 | Success: False\n",
      "Gen  11/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.31273 | Success: False\n",
      "Gen  12/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.29101 | Success: False\n",
      "Gen  13/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.26992 | Success: False\n",
      "Gen  14/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.24960 | Success: False\n",
      "Gen  15/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.23019 | Success: False\n",
      "Gen  16/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.21175 | Success: False\n",
      "Gen  17/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.19434 | Success: False\n",
      "Gen  18/500 | Train Loss: 0.18504 | Val Loss: 0.19789 | σ: 0.17799 | Success: False\n",
      "Gen  19/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.17120 | Success: True\n",
      "Gen  20/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.16368 | Success: False\n",
      "Gen  21/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.15563 | Success: False\n",
      "Gen  22/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.14723 | Success: False\n",
      "Gen  23/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.13864 | Success: False\n",
      "Gen  24/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.12999 | Success: False\n",
      "Gen  25/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.12142 | Success: False\n",
      "Gen  26/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.11300 | Success: False\n",
      "Gen  27/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.10482 | Success: False\n",
      "Gen  28/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.09695 | Success: False\n",
      "Gen  29/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.08942 | Success: False\n",
      "Gen  30/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.08227 | Success: False\n",
      "Gen  31/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.07551 | Success: False\n",
      "Gen  32/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.06916 | Success: False\n",
      "Gen  33/500 | Train Loss: 0.12334 | Val Loss: 0.13716 | σ: 0.06323 | Success: False\n",
      "Gen  34/500 | Train Loss: 0.11959 | Val Loss: 0.11254 | σ: 0.06071 | Success: True\n",
      "Gen  35/500 | Train Loss: 0.11959 | Val Loss: 0.11254 | σ: 0.05795 | Success: False\n",
      "Gen  36/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.05790 | Success: True\n",
      "Gen  37/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.05732 | Success: False\n",
      "Gen  38/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.05626 | Success: False\n",
      "Gen  39/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.05481 | Success: False\n",
      "Gen  40/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.05301 | Success: False\n",
      "Gen  41/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.05094 | Success: False\n",
      "Gen  42/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.04866 | Success: False\n",
      "Gen  43/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.04624 | Success: False\n",
      "Gen  44/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.04371 | Success: False\n",
      "Gen  45/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.04113 | Success: False\n",
      "Gen  46/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.03855 | Success: False\n",
      "Gen  47/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.03598 | Success: False\n",
      "Gen  48/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.03347 | Success: False\n",
      "Gen  49/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.03104 | Success: False\n",
      "Gen  50/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.02869 | Success: False\n",
      "Gen  51/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.02646 | Success: False\n",
      "Gen  52/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.02433 | Success: False\n",
      "Gen  53/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.02233 | Success: False\n",
      "Gen  54/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.02044 | Success: False\n",
      "Gen  55/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.01868 | Success: False\n",
      "Gen  56/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.01705 | Success: False\n",
      "Gen  57/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.01553 | Success: False\n",
      "Gen  58/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.01412 | Success: False\n",
      "Gen  59/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.01283 | Success: False\n",
      "Gen  60/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.01164 | Success: False\n",
      "Gen  61/500 | Train Loss: 0.07939 | Val Loss: 0.08369 | σ: 0.01054 | Success: False\n",
      "Gen  62/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.01004 | Success: True\n",
      "Gen  63/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00951 | Success: False\n",
      "Gen  64/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00897 | Success: False\n",
      "Gen  65/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00842 | Success: False\n",
      "Gen  66/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00788 | Success: False\n",
      "Gen  67/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00734 | Success: False\n",
      "Gen  68/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00682 | Success: False\n",
      "Gen  69/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00631 | Success: False\n",
      "Gen  70/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00582 | Success: False\n",
      "Gen  71/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00536 | Success: False\n",
      "Gen  72/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00493 | Success: False\n",
      "Gen  73/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00452 | Success: False\n",
      "Gen  74/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00413 | Success: False\n",
      "Gen  75/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00377 | Success: False\n",
      "Gen  76/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00344 | Success: False\n",
      "Gen  77/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00313 | Success: False\n",
      "Gen  78/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00284 | Success: False\n",
      "Gen  79/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00258 | Success: False\n",
      "Gen  80/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00234 | Success: False\n",
      "Gen  81/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00212 | Success: False\n",
      "Gen  82/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00192 | Success: False\n",
      "Gen  83/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00173 | Success: False\n",
      "Gen  84/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00156 | Success: False\n",
      "Gen  85/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00141 | Success: False\n",
      "Gen  86/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00127 | Success: False\n",
      "Gen  87/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00115 | Success: False\n",
      "Gen  88/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00103 | Success: False\n",
      "Gen  89/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00093 | Success: False\n",
      "Gen  90/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00084 | Success: False\n",
      "Gen  91/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00075 | Success: False\n",
      "Gen  92/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00068 | Success: False\n",
      "Gen  93/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00061 | Success: False\n",
      "Gen  94/500 | Train Loss: 0.06187 | Val Loss: 0.06316 | σ: 0.00055 | Success: False\n",
      "Gen  95/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00052 | Success: True\n",
      "Gen  96/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00049 | Success: False\n",
      "Gen  97/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00045 | Success: False\n",
      "Gen  98/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00042 | Success: False\n",
      "Gen  99/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00039 | Success: False\n",
      "Gen 100/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00037 | Success: False\n",
      "Gen 101/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00034 | Success: False\n",
      "Gen 102/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00031 | Success: False\n",
      "Gen 103/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00029 | Success: False\n",
      "Gen 104/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00026 | Success: False\n",
      "Gen 105/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00024 | Success: False\n",
      "Gen 106/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00022 | Success: False\n",
      "Gen 107/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00020 | Success: False\n",
      "Gen 108/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00018 | Success: False\n",
      "Gen 109/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00017 | Success: False\n",
      "Gen 110/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00015 | Success: False\n",
      "Gen 111/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00014 | Success: False\n",
      "Gen 112/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00012 | Success: False\n",
      "Gen 113/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00011 | Success: False\n",
      "Gen 114/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00010 | Success: False\n",
      "Gen 115/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00009 | Success: False\n",
      "Gen 116/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00008 | Success: False\n",
      "Gen 117/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00007 | Success: False\n",
      "Gen 118/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00007 | Success: False\n",
      "Gen 119/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00006 | Success: False\n",
      "Gen 120/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00005 | Success: False\n",
      "Gen 121/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00005 | Success: False\n",
      "Gen 122/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00004 | Success: False\n",
      "Gen 123/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00004 | Success: False\n",
      "Gen 124/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00004 | Success: False\n",
      "Gen 125/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00003 | Success: False\n",
      "Gen 126/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00003 | Success: False\n",
      "Gen 127/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00003 | Success: False\n",
      "Gen 128/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00002 | Success: False\n",
      "Gen 129/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00002 | Success: False\n",
      "Gen 130/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00002 | Success: False\n",
      "Gen 131/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00002 | Success: False\n",
      "Gen 132/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00002 | Success: False\n",
      "Gen 133/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 134/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 135/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 136/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 137/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 138/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 139/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 140/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 141/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 142/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00001 | Success: False\n",
      "Gen 143/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 144/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 145/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 146/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 147/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 148/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 149/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 150/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 151/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 152/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 153/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 154/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 155/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 156/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Gen 157/500 | Train Loss: 0.06160 | Val Loss: 0.05498 | σ: 0.00000 | Success: False\n",
      "Early stop: sigma ≈ 0\n",
      "\n",
      "\n",
      "Final Metrics Summary:\n",
      "========================================\n",
      "CMAES | Dataset: mouse.txt | Val R2: -1.123 | MAE: 0.2708 | Eval Calls: 314 | Time: 106.95s\n",
      "Running DE...\n",
      "Starting DE: 3,457 parameters | pop=20 | F=0.5 | CR=0.9 | generations=400\n",
      "Initial best train loss = 0.04979\n",
      "Generation   1/400 | Population mean: 0.18617 | Best train loss: 0.04979 | Val loss: 0.05870 | Pop. STD: 0.152065\n",
      "Generation   2/400 | Population mean: 0.14154 | Best train loss: 0.04979 | Val loss: 0.05870 | Pop. STD: 0.157969\n",
      "Generation   3/400 | Population mean: 0.13097 | Best train loss: 0.04979 | Val loss: 0.05870 | Pop. STD: 0.157099\n",
      "Generation   4/400 | Population mean: 0.11766 | Best train loss: 0.04979 | Val loss: 0.05870 | Pop. STD: 0.158386\n",
      "Generation   5/400 | Population mean: 0.09200 | Best train loss: 0.04979 | Val loss: 0.05870 | Pop. STD: 0.160218\n",
      "[gen 005] new best train loss = 0.040856\n",
      "Generation   6/400 | Population mean: 0.08715 | Best train loss: 0.04086 | Val loss: 0.04548 | Pop. STD: 0.159462\n",
      "Generation   7/400 | Population mean: 0.06713 | Best train loss: 0.04086 | Val loss: 0.04548 | Pop. STD: 0.157425\n",
      "Generation   8/400 | Population mean: 0.06458 | Best train loss: 0.04086 | Val loss: 0.04548 | Pop. STD: 0.157483\n",
      "Generation   9/400 | Population mean: 0.06195 | Best train loss: 0.04086 | Val loss: 0.04548 | Pop. STD: 0.158885\n",
      "[gen 009] new best train loss = 0.039625\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"data/mouse.txt\"\n",
    "plt.figure(figsize=(16,9))\n",
    "print(f\"\\n===== Dataset: {dataset_path} =====\")\n",
    "for alg, cfg in zip([\"adam\", \"cmaes\", \"de\"], [ADAM_CONFIG, CMAES_CONFIG, DE_CONFIG]):\n",
    "    print(f\"Running {alg.upper()}...\")\n",
    "    result = run_experiment(alg, dataset_path, cfg)\n",
    "    summarize_results(result)\n",
    "    all_results.append(result)\n",
    "    plot_learning_curves(result[\"history\"], label=alg)\n",
    "\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'results/curves_{Path(dataset_path).stem}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in all_results:\n",
    "    summarize_results(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adam-vs-evo-iOHOjByi-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
